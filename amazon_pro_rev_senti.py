# -*- coding: utf-8 -*-
"""Amazon_pro_rev_senti.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18ADVYIBkFM6wrWsyXodsvEyGlJ9yAvMK
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os

#NLP Packages
import nltk
from nltk import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter
from wordcloud import WordCloud

# PAckages for models
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score

# packages for avoiding warning data. 
from pylab import rcParams
import warnings
warnings.filterwarnings("ignore")
rcParams['figure.figsize'] = 14, 6
plt.style.use('ggplot')

"""**Reading and doing basic data analysis of data**

"""

amazon_reviews = pd.read_csv('https://media.githubusercontent.com/media/juliandariomirandacalle/NLP_Notebooks/master/01-Introduction_NLP/Customer_Reviews.csv')
amazon_reviews.head(3)

amazon_reviews.shape

"""**Looking for number of words per** **review**"""

words_per_review = amazon_reviews.Text.apply(lambda x:len(x.split()))
words_per_review.hist(bins = 100)
plt.xlabel("len of Review")
plt.ylabel("Freq")
plt.show()
print("data in words_per_review")
words_per_review

print("Average_words_per_review: ",words_per_review.mean())

"""**Distribution of the ratings-column.**"""

percent_val = 100*amazon_reviews['Score'].value_counts()/len(amazon_reviews)
percent_val

percent_val.plot.bar()
plt.show()

word_cloud_text = ''.join(amazon_reviews['Text'])
wordcloud = WordCloud(max_font_size=100,max_words=100,background_color="white",
                      scale = 10, width=800,height=400).generate(word_cloud_text)
plt.figure()
plt.imshow(wordcloud,interpolation="bilinear")
plt.axis("off")
plt.show()

"""Standardizing the ratings for sentiment analysis (5 mts)"""

# Mapping the ratings
amazon_reviews['Sentiment_rating'] = np.where(amazon_reviews.Score > 3,1,0)

## Removing neutral reviews 
amazon_reviews = amazon_reviews[amazon_reviews.Score != 3]

# Printing the counts of each class
amazon_reviews['Sentiment_rating'].value_counts()

amazon_reviews.Sentiment_rating.value_counts().plot.bar()
plt.show()

"""Pre-processing
1. Converting words to lower/upper case
"""

amazon_reviews['reviews_text_new'] = amazon_reviews['Text'].str.lower()

from nltk import word_tokenize
import nltk
nltk.download('punkt')
# Word tokenization example:
word_tokenize("DPhi Bootcamp rules. It is awesome :D")

# For reviews not converted to lowe case
token_lists = [word_tokenize(each) for each in amazon_reviews['Text']]
tokens = [item for sublist in token_lists for item in sublist]
print("Number of unique tokens then: ",len(set(tokens)))

# For reviews converted to lowe case
token_lists_lower = [word_tokenize(each) for each in amazon_reviews['reviews_text_new']]
tokens_lower = [item for sublist in token_lists_lower for item in sublist]
print("Number of unique tokens now: ",len(set(tokens_lower)))

"""2. Removing special characters"""

### Selecting non alpha numeric charactes that are not spaces
spl_chars = amazon_reviews['reviews_text_new'].apply(lambda review: 
                                                     [char for char in list(review) if not char.isalnum() and char != ' '])

## Getting list of list into a single list
flat_list = [item for sublist in spl_chars for item in sublist]

## Unique special characters
set(flat_list)

review_backup = amazon_reviews['reviews_text_new'].copy()
amazon_reviews['reviews_text_new'] = amazon_reviews['reviews_text_new'].str.replace(r'[^A-Za-z0-9 ]+', ' ')

print("- Old Review -")
print(review_backup.values[5])
print("\n- New Review -")
print(amazon_reviews['reviews_text_new'][5])

token_lists = [word_tokenize(each) for each in amazon_reviews['Text']]
tokens = [item for sublist in token_lists for item in sublist]
print("Number of unique tokens then: ",len(set(tokens)))

token_lists = [word_tokenize(each) for each in amazon_reviews['reviews_text_new']]
tokens = [item for sublist in token_lists for item in sublist]
print("Number of unique tokens now: ",len(set(tokens)))

"""3. Stopwords and high/low frequency words"""

from nltk.corpus import stopwords
nltk.download('stopwords')

print('Available languages for NLTK v.3.4.5: ')
print(stopwords.fileids())

noise_words = []
eng_stop_words = stopwords.words('english')
eng_stop_words

"""example of removing stopwords:"""

stop_words = set(eng_stop_words)
without_stop_words = []
stopword = []
sentence = amazon_reviews['reviews_text_new'][0]
words = nltk.word_tokenize(sentence)

for word in words:
    if word in stop_words:
        stopword.append(word)
    else:
        without_stop_words.append(word)

print('-- Original Sentence --\n', sentence)
print('\n-- Stopwords in the sentence --\n', stopword)
print('\n-- Non-stopwords in the sentence --\n', without_stop_words)

def stopwords_removal(stop_words, sentence):
    return [word for word in nltk.word_tokenize(sentence) if word not in stop_words]

amazon_reviews['reviews_text_nonstop'] = amazon_reviews['reviews_text_new'].apply(lambda row: stopwords_removal(stop_words, row))
amazon_reviews[['reviews_text_new','reviews_text_nonstop']]

"""4. Stemming & lemmatization"""

from nltk.stem import PorterStemmer, LancasterStemmer # Common stemmers
from nltk.stem import WordNetLemmatizer # Common Lematizer
nltk.download('wordnet')
from nltk.corpus import wordnet

porter = PorterStemmer()
lancaster = LancasterStemmer()
lemmatizer = WordNetLemmatizer()

print("Lancaster Stemmer")
print(lancaster.stem("trouble"))
print(lancaster.stem("troubling"))
print(lancaster.stem("troubled"))

# Provide a word to be lemmatized
print("WordNet Lemmatizer")
print(lemmatizer.lemmatize("trouble", wordnet.NOUN))
print(lemmatizer.lemmatize("troubling", wordnet.VERB))
print(lemmatizer.lemmatize("troubled", wordnet.VERB))

"""Bag-of-words"""

# The following code creates a word-document matrix.
from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer()
X = vec.fit_transform(amazon_reviews['reviews_text_new'])
df = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())
df.head()

"""use this to create a bag of words from the reviews, excluding the noise words we identified earlier:


"""

### Creating a python object of the class CountVectorizer

bow_counts = CountVectorizer(tokenizer= word_tokenize, # type of tokenization
                             stop_words=noise_words, # List of stopwords
                             ngram_range=(1,1)) # number of n-grams

bow_data = bow_counts.fit_transform(amazon_reviews['reviews_text_new'])

bow_data.head()

X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data, # Features
                                                                    amazon_reviews['Sentiment_rating'], # Target variable
                                                                    test_size = 0.2, # 20% test size
                                                                    random_state = 0) # random state for replication purposes

y_test_bow.value_counts()/y_test_bow.shape[0]

"""**Applying logistic regression:**

Let's train the model on our training data and run the resulting model on our test data:
"""

### Training the model 
lr_model_all = LogisticRegression() # Logistic regression
lr_model_all.fit(X_train_bow, y_train_bow) # Fitting a logistic regression model

## Predicting the output
test_pred_lr_all = lr_model_all.predict(X_test_bow) # Class prediction

## Calculate key performance metrics
print("F1 score: ", f1_score(y_test_bow, test_pred_lr_all))

############### RandomForest Classifier applied. 
rfc = RandomForestClassifier(n_estimators=100)
rfc.fit(X_train_bow,y_train_bow)

y_pred=rfc.predict(X_test_bow)

print("F1 score: ",f1_score(y_test_bow,y_pred))

"""modifying the set of features in the model to include bigrams, trigrams, and 4-grams:"""

### Changes with respect to the previous code
### 1. Increasing the n-grams from just having 1-gram to (1-gram, 2-gram, 3-gram, and 4-gram)
### 2. Including the stopwords in the bag of words features

bow_counts = CountVectorizer(tokenizer= word_tokenize,
                             ngram_range=(1,4))

bow_data = bow_counts.fit_transform(amazon_reviews.reviews_text_new)

# Notice the increase in features with inclusion of n-grams
bow_data

X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data,
                                                                    amazon_reviews['Sentiment_rating'],
                                                                    test_size = 0.2,
                                                                    random_state = 0)

#### LOGISTIC REGRESSION
# Defining and training the model
lr_model_all_new = LogisticRegression(max_iter = 200)
lr_model_all_new.fit(X_train_bow, y_train_bow)

# Predicting the results
test_pred_lr_all = lr_model_all_new.predict(X_test_bow)

print("F1 score: ", f1_score(y_test_bow,test_pred_lr_all))

rfc_new = RandomForestClassifier(n_estimators=100)
rfc_new.fit(X_train_bow,y_train_bow)

y_pred=rfc_new.predict(X_test_bow)

print("F1 score: ",f1_score(y_test_bow,y_pred))

"""The F1-score has jumped slightly for logistic regression. This is an example of what simple hyperparameter tuning and input feature modification can do to the overall performance. We can even get interpretable features from this in terms of what contributed the most to positive and negative sentiment:"""

lr_weights = pd.DataFrame(list(zip(bow_counts.get_feature_names(), # get all the n-gram feature names
                                   lr_model_all_new.coef_[0])), # get the logistic regression coefficients
                          columns= ['words','weights']) # defining the colunm names

lr_weights.sort_values(['weights'], ascending = False)[:15] # top-15 more important features for positive reviews

lr_weights.sort_values(['weights'], ascending = False)[-15:] # top-15 more important features for negative reviews

"""**TF-IDF model**

re-featurize our original set of reviews based on TF-IDF and split the resulting features into train and test sets:
"""

from sklearn.feature_extraction.text import TfidfVectorizer

### Creating a python object of the class CountVectorizer
tfidf_counts = TfidfVectorizer(tokenizer= word_tokenize, # type of tokenization
                               stop_words=noise_words, # List of stopwords
                               ngram_range=(1,1)) # number of n-grams

tfidf_data = tfidf_counts.fit_transform(amazon_reviews['reviews_text_new'])

tfidf_data

X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_data,
                                                                            amazon_reviews['Sentiment_rating'],
                                                                            test_size = 0.2,
                                                                            random_state = 0)

"""Applying logistic regression to TF-IDF features"""

### Setting up the model class
lr_model_tf_idf = LogisticRegression()

## Training the model 
lr_model_tf_idf.fit(X_train_tfidf,y_train_tfidf)

## Prediciting the results
test_pred_lr_all = lr_model_tf_idf.predict(X_test_tfidf)

## Evaluating the model
print("F1 score: ",f1_score(y_test_tfidf, test_pred_lr_all))

"""Applying RandomForest classifier to TF-IDF features"""

rfc_tf = RandomForestClassifier(n_estimators=100)
rfc_tf.fit(X_train_tfidf,y_train_tfidf)

y_pred=rfc_tf.predict(X_test_tfidf)

print("F1 score: ",f1_score(y_test_tfidf,y_pred))